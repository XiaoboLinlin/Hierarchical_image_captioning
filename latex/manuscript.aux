\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vaswani2017attention}
\citation{FasterRCNN}
\citation{BottomUp}
\citation{ViT}
\citation{liu2021swin}
\citation{liu2021cptr}
\citation{COCO}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{brf}{\backcite{vaswani2017attention}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{FasterRCNN}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{BottomUp}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{ViT}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{liu2021swin}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{liu2021cptr}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{COCO}{{1}{1}{section.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Approach and Methods}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em.\nobreakspace  {}Hierarchical Encoder}{2}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Overview of our hierarchical ViT-CNN architecture for image captioning. The model combines a multi-scale encoder that processes visual information at different resolutions with a Transformer decoder that generates captions by attending to these features.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:architecture}{{1}{3}{Overview of our hierarchical ViT-CNN architecture for image captioning. The model combines a multi-scale encoder that processes visual information at different resolutions with a Transformer decoder that generates captions by attending to these features}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em.\nobreakspace  {}Transformer Decoder}{3}{subsection.2.2}\protected@file@percent }
\citation{liu2101cptr}
\citation{COCO}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\hskip -1em.\nobreakspace  {}Training and Optimization}{4}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Experiments and Results}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Experimental Setup}{4}{subsection.3.1}\protected@file@percent }
\@writefile{brf}{\backcite{liu2101cptr}{{4}{3.1}{subsection.3.1}}}
\@writefile{brf}{\backcite{COCO}{{4}{3.1}{subsection.3.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Evaluation Metrics}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Quantitative Results}{4}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training and validation loss curves over 50 epochs. The consistent convergence pattern demonstrates the stability of our hierarchical model during training, with minimal overfitting.}}{5}{figure.2}\protected@file@percent }
\newlabel{fig:loss_plot}{{2}{5}{Training and validation loss curves over 50 epochs. The consistent convergence pattern demonstrates the stability of our hierarchical model during training, with minimal overfitting}{figure.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance metrics on MS COCO test set with mean values and standard deviations.}}{5}{table.1}\protected@file@percent }
\newlabel{tab:metrics}{{1}{5}{Performance metrics on MS COCO test set with mean values and standard deviations}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}\hskip -1em.\nobreakspace  {}Qualitative Analysis}{5}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Discussion and Conclusions}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.\nobreakspace  {}Limitations and Future Work}{5}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Team Contribution (see Table \ref  {tab:contributions})}{5}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Attention visualization showing how our hierarchical model focuses on different image regions when generating caption words. The visualization depicts the encoder-decoder attention weights from the final decoder layer, averaged across all attention heads, providing a case study of the model's focus during generation.}}{6}{figure.3}\protected@file@percent }
\newlabel{fig:attention}{{3}{6}{Attention visualization showing how our hierarchical model focuses on different image regions when generating caption words. The visualization depicts the encoder-decoder attention weights from the final decoder layer, averaged across all attention heads, providing a case study of the model's focus during generation}{figure.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Contributions of team members.}}{6}{table.2}\protected@file@percent }
\newlabel{tab:contributions}{{2}{6}{Contributions of team members}{table.2}{}}
\bibstyle{unsrt}
\bibdata{egbib}
\bibcite{vaswani2017attention}{1}
\bibcite{FasterRCNN}{2}
\bibcite{BottomUp}{3}
\bibcite{ViT}{4}
\bibcite{liu2021swin}{5}
\bibcite{liu2021cptr}{6}
\bibcite{COCO}{7}
\bibcite{liu2101cptr}{8}
\gdef \@abspage@last{7}
